{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics\n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required.\n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas.\n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it.\n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link.\n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell.\n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. _Make sure to answer every question marked with a `Q:` for full credit._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "# data_location = \"/users/chandler/dropbox/teaching/repos/ads-tm-api-scrape/\"\n",
    "data_location = Path(\"./datasets\")\n",
    "\n",
    "# These subfolders should still work if you correctly stored the\n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, print number of tokens, number of unique tokens,\n",
    "    number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity),\n",
    "    and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "    of unique tokens, lexical diversity, and number of characters.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def count_char_of_token(token_list):\n",
    "        total = 0\n",
    "        for token in token_list:\n",
    "            total += len(token)\n",
    "\n",
    "        return total\n",
    "\n",
    "    def lex_diversity(unique, total):\n",
    "        lex_div = 0\n",
    "        if total > 0:\n",
    "            lex_div = unique / total\n",
    "\n",
    "        return lex_div\n",
    "\n",
    "    # Fill in the correct values here.\n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = lex_diversity(num_unique_tokens, num_tokens)\n",
    "    num_characters = count_char_of_token(tokens)\n",
    "\n",
    "    most_common_tokens = Counter(tokens).most_common(5)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "\n",
    "        # print the five most common tokens\n",
    "        print(most_common_tokens)\n",
    "\n",
    "    return [num_tokens, num_unique_tokens, lexical_diversity, num_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "[('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert descriptive_stats(text, verbose=True)[0] == 13\n",
    "assert descriptive_stats(text, verbose=False)[1] == 9\n",
    "assert abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02\n",
    "assert descriptive_stats(text, verbose=False)[3] == 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code?\n",
    "\n",
    "A: Assertion statements are typically used in unit testing. This allows the developer to ensure the code is returning the expected values by creating assert statements that given a specific input, this is what I am expecting this function to provide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well.\n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "# create path to lyrics\n",
    "lyrics_path = os.path.join(data_location, lyrics_folder)\n",
    "\n",
    "# get list of artists in lyrics folder\n",
    "artists = os.listdir(lyrics_path)\n",
    "\n",
    "# initialize rows list for dataframe\n",
    "rows = []\n",
    "\n",
    "for artist in artists:\n",
    "    # create path to song lyrics\n",
    "    song_lyrics_path = os.path.join(lyrics_path, artist)\n",
    "\n",
    "    # iterate through all song files in the directory\n",
    "    for songs in os.listdir(song_lyrics_path):\n",
    "        # create path to song file\n",
    "        file_path = os.path.join(song_lyrics_path, songs)\n",
    "\n",
    "        # read txt file to lyrics var\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # create regex to capture title between double quotes\n",
    "        match = re.match(r'^\"(.*)\"$', lines[0].strip())\n",
    "        if match:\n",
    "            song_title = match.group(1)\n",
    "        else:\n",
    "            # fallback to first line as title\n",
    "            song_title = lines[0].strip()\n",
    "\n",
    "        # save rest of lines to lyrics\n",
    "        lyrics = \"\".join(lines[1:]).strip()\n",
    "\n",
    "        rows.append({\"artist\": artist, \"song_title\": song_title, \"lyrics\": lyrics})\n",
    "\n",
    "# create dataframe from rows var\n",
    "df_lyrics = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "\n",
    "twitter_path = os.path.join(data_location, twitter_folder)\n",
    "\n",
    "twitter_files = os.listdir(twitter_path)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Regex101 was used to create the regex\n",
    "# ChatGPT was used to help create the syntax to make the regex useable with\n",
    "# Python\n",
    "LINE_RE = re.compile(\n",
    "    r\"^(?P<screen_name>[^\\t]*)\\t\"\n",
    "    r\"(?P<name>[^\\t]*)\\t\"\n",
    "    r\"(?P<id>\\d+)\\t\"\n",
    "    r\"(?P<location>[^\\t]*)\\t\"\n",
    "    r\"(?P<followers_count>\\d+)\\t\"\n",
    "    r\"(?P<friends_count>\\d+)\\t\"\n",
    "    r\"(?P<description>.*)$\"\n",
    ")\n",
    "\n",
    "\n",
    "for file in twitter_files:\n",
    "    # filter for data file that has description column\n",
    "    if \"_data.txt\" in file and file != \".DS_Store\":\n",
    "        # save robynkonichiwa as robyn\n",
    "        artist = file.split(\"_\")[0].replace(\"konichiwa\", \"\")\n",
    "        file_path = os.path.join(twitter_path, file)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            # skip header row by assigning to `_`\n",
    "            _ = f.readline()\n",
    "            # iterate through remaining lines\n",
    "            for line in f:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                m = LINE_RE.match(line)\n",
    "                if m:\n",
    "                    desc = m.group(\"description\")\n",
    "                    # only append if desc is not blank\n",
    "                    if desc:\n",
    "                        rows.append({\"artist\": artist, \"description\": desc})\n",
    "\n",
    "\n",
    "df_twitter = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation)  # speeds up comparison\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    sentence = \"\"\n",
    "    for ch in text:\n",
    "        if ch not in punctuation:\n",
    "            sentence += ch\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    tkn = []\n",
    "    for token in tokens:\n",
    "        if token not in sw:\n",
    "            tkn.append(token)\n",
    "    return tkn\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "    # tokenize by splitting by whitespace\n",
    "    tokens = text.split()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b327033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe already exists! Loading...\n"
     ]
    }
   ],
   "source": [
    "# create your clean twitter data here\n",
    "twitter_pkl = data_location / \"df_twitter_cleaned.pkl\"\n",
    "if not twitter_pkl.exists():\n",
    "    print(\"Cleaning and saving data...\")\n",
    "    df_twitter[\"description\"] = df_twitter[\"description\"].apply(clean_text)\n",
    "    df_twitter[\"no_sw\"] = df_twitter[\"description\"].apply(remove_stopwords)\n",
    "\n",
    "    # save cleaned DF to pickle\n",
    "    df_twitter.to_pickle(twitter_pkl)\n",
    "\n",
    "else:\n",
    "    print(\"Dataframe already exists! Loading...\")\n",
    "    df_twitter = pd.read_pickle(twitter_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe already exists! Loading...\n"
     ]
    }
   ],
   "source": [
    "# create your clean lyrics data here\n",
    "lyrics_pkl = data_location / \"df_lyrics_cleaned.pkl\"\n",
    "if not lyrics_pkl.exists():\n",
    "    print(\"Cleaning and saving data... \")\n",
    "    df_lyrics[\"lyrics\"] = df_lyrics[\"lyrics\"].apply(clean_text)\n",
    "    df_lyrics[\"no_sw\"] = df_lyrics[\"lyrics\"].apply(remove_stopwords)\n",
    "\n",
    "    # save cleaned DF to pickle\n",
    "    df_lyrics.to_pickle(lyrics_pkl)\n",
    "\n",
    "else:\n",
    "    print(\"Dataframe already exists! Loading...\")\n",
    "    df_lyrics = pd.read_pickle(lyrics_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Stats Cher Lyrics:\n",
      "\n",
      "There are 35233 tokens in the data.\n",
      "There are 3684 unique tokens in the data.\n",
      "There are 169244 characters in the data.\n",
      "The lexical diversity is 0.105 in the data.\n",
      "[('love', 966), ('im', 511), ('know', 480), ('dont', 430), ('youre', 332)]\n",
      "\n",
      "Descriptive Stats Robyn Lyrics:\n",
      "\n",
      "There are 15041 tokens in the data.\n",
      "There are 2139 unique tokens in the data.\n",
      "There are 72804 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n",
      "[('know', 305), ('im', 299), ('dont', 297), ('love', 269), ('got', 249)]\n",
      "\n",
      "Descriptive Stats Cher Twitter:\n",
      "\n",
      "There are 15670504 tokens in the data.\n",
      "There are 1517718 unique tokens in the data.\n",
      "There are 92951718 characters in the data.\n",
      "The lexical diversity is 0.097 in the data.\n",
      "[('love', 214576), ('im', 139098), ('life', 122980), ('music', 88177), ('de', 72974)]\n",
      "\n",
      "Descriptive Stats Robyn Twitter:\n",
      "\n",
      "There are 1495526 tokens in the data.\n",
      "There are 252736 unique tokens in the data.\n",
      "There are 9127817 characters in the data.\n",
      "The lexical diversity is 0.169 in the data.\n",
      "[('music', 15147), ('love', 11677), ('im', 9051), ('och', 7922), ('life', 7383)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1495526, 252736, 0.16899472158959455, 9127817]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "def get_token_list(dataframe, artist_name, column):\n",
    "    token_list = []\n",
    "    artist_rows = dataframe[dataframe[\"artist\"] == artist_name]\n",
    "    lists = artist_rows[column].tolist()\n",
    "    for list in lists:\n",
    "        token_list.extend(list)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "# lyrics\n",
    "cher_lyrics = get_token_list(df_lyrics, \"cher\", \"no_sw\")\n",
    "robyn_lyrics = get_token_list(df_lyrics, \"robyn\", \"no_sw\")\n",
    "\n",
    "print(\"\\nDescriptive Stats Cher Lyrics:\\n\")\n",
    "descriptive_stats(cher_lyrics)\n",
    "print(\"\\nDescriptive Stats Robyn Lyrics:\\n\")\n",
    "descriptive_stats(robyn_lyrics)\n",
    "\n",
    "# twitter\n",
    "cher_twitter = get_token_list(df_twitter, \"cher\", \"no_sw\")\n",
    "robyn_twitter = get_token_list(df_twitter, \"robyn\", \"no_sw\")\n",
    "\n",
    "print(\"\\nDescriptive Stats Cher Twitter:\\n\")\n",
    "descriptive_stats(cher_twitter)\n",
    "print(\"\\nDescriptive Stats Robyn Twitter:\\n\")\n",
    "descriptive_stats(robyn_twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data?\n",
    "\n",
    "A: It would be different because the top 5 words would be comprised of the stopwords.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs?\n",
    "\n",
    "A: Honestly, I didn't have any beliefs about lexical diversity between the artists. Looking at the statistical data for lyrics, Cher has a lower lexical diversity compared to Robyn, but Cher also has twice as many words (tokens) in the dataset compared to Robyn which would lead to a lower lexical diversity. The Twitter data are tweets from their fan base and have about an equal amount of tokens making for a better comparison. Robyn has a higher lexical diversity compared to Cher for tweets showing that Europeans might have a better grasp of utilizing all aspects of the English language. On the other hand, the higher lexical diversity for Robyn could also represent the diversity of English language used from different countries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist.\n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens)\n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert emoji.is_emoji(\"‚ù§Ô∏è\")\n",
    "assert not emoji.is_emoji(\":-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis üòÅ\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "269cd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def get_emojis(list):\n",
    "    emoji_list = []\n",
    "    for l in list:\n",
    "        if emoji.is_emoji(l):\n",
    "            emoji_list.append(l)\n",
    "    return emoji_list\n",
    "\n",
    "\n",
    "cher_emojis = get_emojis(cher_twitter)\n",
    "robyn_emojis = get_emojis(robyn_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a8fd14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher's 10 most common emojis in Twitter desriptions are:\n",
      "\n",
      "[('‚ù§Ô∏è', 14720), ('üè≥Ô∏è\\u200düåà', 14147), ('‚ô•', 10158), ('‚ù§', 9662), ('‚ú®', 8344), ('üåà', 5483), ('üá∫üá∏', 3699), ('üíô', 3685), ('üíú', 3499), ('üåä', 3291)]\n",
      "\n",
      "\n",
      "Robyn's 10 most common emojis in Twitter desriptions are:\n",
      "\n",
      "[('üè≥Ô∏è\\u200düåà', 1703), ('‚ô•', 1167), ('‚ù§Ô∏è', 987), ('‚ú®', 751), ('‚ù§', 653), ('üåà', 570), ('üé∂', 272), ('üéß', 213), ('üñ§', 212), ('üíú', 205)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cher's 10 most common emojis in Twitter desriptions are:\\n\")\n",
    "print(Counter(cher_emojis).most_common(10))\n",
    "\n",
    "print(\"\\n\\nRobyn's 10 most common emojis in Twitter desriptions are:\\n\")\n",
    "print(Counter(robyn_emojis).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"artist\": [\"Artist 1\"] * num_replicates + [\"Artist 2\"] * num_replicates,\n",
    "        \"length\": np.concatenate(\n",
    "            (\n",
    "                np.random.poisson(125, num_replicates),\n",
    "                np.random.poisson(150, num_replicates),\n",
    "            )\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "df.groupby(\"artist\")[\"length\"].plot(kind=\"hist\", density=True, alpha=0.5, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting.\n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on?\n",
    "\n",
    "A:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def tokenize_lyrics(lyric):\n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return [item.lower() for item in collapse_whitespace.split(lyric)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads509",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
